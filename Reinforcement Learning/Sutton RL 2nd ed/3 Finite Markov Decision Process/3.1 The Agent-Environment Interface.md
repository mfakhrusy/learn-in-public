
![[Pasted image 20250513053958.png]]

# Chapter 3.1: The Agent-Environment Interface

(summarized by google gemini)

Chapter 3.1 of "Finite Markov Decision Processes" introduces the fundamental concept of the **agent-environment interface** within the framework of Markov Decision Processes (MDPs).

The core idea is that a learning and decision-making entity, known as the **agent**, interacts with everything outside of it, termed the **environment**. This interaction is continuous, with the agent choosing **actions** and the environment, in turn, responding to these actions and presenting new situations or **states** to the agent. Crucially, the environment also provides **rewards**, which are numerical values that the agent aims to maximize over time through its action selections.

This interaction unfolds over a sequence of discrete **time steps** ($t=0, 1, 2, 3,...$). At each time step $t$:
* The agent receives a representation of the environment's current state, $S_t$.
* Based on this state, the agent selects an action, $A_t$.
* One time step later, as a result of its action, the agent receives a numerical reward, $R_{t+1}$, and transitions to a new state, $S_{t+1}$.

This sequence of interactions ($S_0, A_0, R_1, S_1, A_1, R_2, ...$) forms a **trajectory**.

In a **finite MDP**, the sets of states, actions, and rewards are all finite. The probability of transitioning to a particular next state $s'$ and receiving a reward $r$, given the current state $s$ and action $a$, is defined by the **dynamics function** $p(s',r|s,a)$. This function essentially defines the model of the environment.

A key assumption is the **Markov Property**, which states that the future is independent of the past given the present state and action. In other words, the current state $S_t$ must encapsulate all information from past interactions that is relevant for future decision-making.

The chapter emphasizes that the MDP framework is abstract and flexible, applicable to a wide variety of problems. The definitions of states and actions can vary greatly depending on the task, ranging from low-level sensor readings and motor controls to high-level conceptual decisions. The boundary between the agent and the environment is not necessarily a physical one but is defined by what the agent can arbitrarily change; anything it cannot arbitrarily change is considered part of the environment. Even the reward computation, though it might occur within the agent's physical body, is considered external as it defines the task.

Ultimately, any problem of learning goal-directed behavior can be abstracted into three signals: actions (choices), states (basis for choices), and rewards (defining the goal). While the representation of states and actions is crucial and currently more of an art than a science, the chapter's primary focus is on the general principles of learning behavior once these representations are established.

The section concludes with examples like a bioreactor and a pick-and-place robot to illustrate how various real-world problems can be framed within this agent-environment interface of an MDP.

----------------

My own notes

MDP is abstract and flexible.

actions:

- can be low-level like voltages applied to a robot arm
- can be high-level like whether or not to have a lunch or to go to grad school

states

- can be low level like sensor reading
- can be high-level like symbolic descriptions of objects in a room

in general: actions can be any decision we want to learn how to make, states can be anything we can know that might be useful in making them.

Boundary between agent and environment is not the same as physical boundary (e.g of a robot's arm or animal's body)

for example: MDP that applies to a person -> muscles, skeleton, and sensory organs should be considered the environment, rather than the agent itself.

Rewards can also be computed inside the physical bodies of a natural/artificial learning system, but are considered external to an agent.

General rule: anything that can not be changed arbitrarily by the agent is considered to be outside (environment).

 agent-environment boundary represent the limit of the agent's absolute control, not its knowledge.