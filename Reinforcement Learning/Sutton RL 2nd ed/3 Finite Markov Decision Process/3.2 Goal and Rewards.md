
Using reward signal to formalize the idea (definition) of a goal is one of the main features of RL.

example: to make a robot learn to walk, reward can be introduced proportional to how far they move.

agents will always learn to maximize the rewards -> if we want it to do something, then we have to make maximizing it will also achieve our goals.

in chess, give reward when agent actually wins, not when it's eating opponent's pieces (because it can eat, but it may lose!)



