MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.

In banding, we estimated the value $q_*(a)$ for each action $a$. In MDP, we estimate the value of $q_*(s,a)$ of each action $a$ and each state $s$, or we estimate the value $v_*(s)$ of each state given optimal action selections.

MDP: mathematically idealized form of RL problem which precise theoretical statements can be made.