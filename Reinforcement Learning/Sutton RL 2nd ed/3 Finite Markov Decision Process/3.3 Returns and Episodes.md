
we seek to maximize the expected return, where return, denoted $G_{t}$, is defined as some specific function of the reward sequence

simplest case, it returns the sum

$$G_{t} \doteq R_{t+1} + R_{t+2} + ... + R_{T}$$

where T is the final time step

this make sense when agent-env interaction breaks naturally into subsequences, which we call episodes (e.g play of a game, trips through maze). each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states.

so, the next episode begin independently. In a board game, this mean a full complete game is a single episode.

Many other task do not naturally episodic. For example: long running on-going process-control task, or robot with a long life span. This is called continuing tasks.

Gt would be problematic, because T is infinity.

$$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Where $\gamma$ is a parameter, $0 \leq \gamma \leq 1$, called the discount rate.

discount rate -> determines the present value of future rewards. A reward received k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it's received immediately. (basically, immediate reward will be much bigger)

If $\gamma = 0$, the agent is myopic -> it seeks immediate reward instantly. The nearer the $\gamma$ to 1, the more "far-sighted" it is (seek more future rewards).



