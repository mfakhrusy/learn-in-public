RL is learning what to do to maximize a numerical reward signal.

learner is not told which actions to take, but must discover which actions yield the most reward by trying them. (trial and error)

actions may not only affect the immediate reward, but also the next situation and through that, all subsequent reward. (delayed reward)

problem of RL is formalized using ideas from dynamical systems theory,  specifically as the optimal control of incompletely-known MDP (markov decision process)

Learning agent must be:
- sense the sate of its env
- take actions that affect the state
- have a goal/goals relating to the state of the env


MDP also just include 3 aspects: sensation, action, goal

RL is different than supervised learning (learning from a labeled examples --training set -- provided by a knowledgeable external supervisor -> objective of this is for the system to generalize/extrapolate its responses so it acts correctly in situations not present in the training set)

It's often impractical to obtain example of desired behaviour that are both correct and representative of all the situations which the agent has to act -- agent must learn how to act in uncharted territory.

RL is also different from unsupervised learning (finding structures hidden in unlabeled data). RL is different because RL is trying to maximize a reward signal, not finding hidden structure, but RL may find uncovering hidden structure useful, but not complete.

challenges in RL is the trade off between exploitation and exploration

- To obtain a lot of reward -> exploitation of successful action
- To discover such action -> exploration of untried action

need to balance both.

the whole goal-directed agent interacting with uncertain environment is taken as a whole problem -- not just subproblem of it.

- goal
- state sensing
- action

even from the very beginning (where movement is uncertain), agent has to already operated.
