
1. Policy
2. Reward Signal
3. Value function
4. (optional) Model of the environment

Policy: 

- agent way of behaving at given time
- mapping from perceived states of the env -> actions to be taken
- in psychology, stimulus-response rules or associations
- may be a simple function or a lookup table, or can involve extensive computation (such as search process)
- may be stochastic, specifying probabilities of each action

reward signal:

- defines the goal of RL problem
- each timestep, agent send RL agent a single number called reward
- agent sole objective: maximize total reward in the long run
- reward signal defines what are good and bad
- metaphor: in biology, similar to how pleasure and pain works
- tell what is good in an immediate sense

value function:

- indicates what is good in the long run
- value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state
- values indicate long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states
- e.g: a state might yield low immediate reward, but have a high value because it is regularly followed by a high value state (or vise versa)

We week actions that bring the highest values, not reward, and it's harder to determine values (long-term) than reward (immediate). Values must keep re-estimated from the sequences of observations an agent makes over its entire lifetime.

One of the most important component for all RL algo is a method for efficiently estimating values (value estimation).

model of the environment

- mimic the behavior of the env
- or generally, allows inference to be made about how the env will behave
- example: given state and action, model might predict the resultant next state and next reward.
- model is used for planning (predict future before they are experienced)
- model-based method: use model
- model-free method: trial and error -> opposite of planning